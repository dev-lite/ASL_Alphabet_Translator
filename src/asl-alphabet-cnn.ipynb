{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":29550,"sourceType":"datasetVersion","datasetId":23079},{"sourceId":11280148,"sourceType":"datasetVersion","datasetId":7052293}],"dockerImageVersionId":30918,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport os\nimport glob\nimport random\nimport time\nimport numpy as np\nfrom numpy.linalg import norm\nimport pandas as pd \nimport cv2\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nfrom torch.utils.data import Dataset, DataLoader, random_split\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_squared_error, confusion_matrix, ConfusionMatrixDisplay\nimport seaborn as sns\n\n# Input data files are available in the read-only \"../input/\" directory\n\ndataset_path = \"/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train\"\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"execution_failed":"2025-04-05T02:57:52.335Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def apply_clahe(image):\n  \n    \"\"\" Apply CLAHE (Contrast Limited Adaptive Histogram Equalization) \"\"\"\n    \n    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)  # Convert to grayscale\n    clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(10,10))  # Create CLAHE object\n    equalized = clahe.apply(gray)  # Apply CLAHE\n    \n    return equalized ","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-05T02:57:52.335Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ASLDataset(Dataset):\n  \"\"\"ASL dataset.\"\"\"\n\n  def __init__(self, root_dir, transform=None):\n\n    \"\"\"\n    Args:\n      root_dir: Image directory\n      transform: Optional transform to be applied.\n    \"\"\"\n\n    self.root_dir = root_dir\n    self.transform = transform\n\n    # Locate files in the dataset and assign labels (label == folder name)\n    self.image_paths = glob.glob(os.path.join(root_dir, '**/*.jpg'), recursive=True)\n    self.labels = [os.path.basename(os.path.dirname(path)) for path in self.image_paths] \n    \n    # Convert between labels and integers\n    self.label_to_index = {label: idx for idx, label in enumerate(sorted(set(self.labels)))}\n    self.index_to_label = {idx: label for label, idx in self.label_to_index.items()}\n    self.labels = [self.label_to_index[label] for label in self.labels]\n\n  def __len__(self):\n    return len(self.image_paths)\n\n  def __getitem__(self, idx):\n\n    \"\"\" \n    Args:\n      idx: Index of the item to get\n    \"\"\"\n      \n    path = self.image_paths[idx]\n    label = self.labels[idx]\n\n    image = cv2.imread(path)  # Load with OpenCV\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n    image = apply_clahe(image)\n\n    image = np.array(image, dtype=np.uint8)\n    # Resize image\n    image = cv2.resize(image, (64, 64))\n\n    # Normalize to [-1, 1]\n    image = image.astype(np.float32) / 255.0\n    image = (image - 0.5) / 0.5 \n\n    # Convert to torch tensor and permute to (C, H, W)--use this if processing color images\n    #image = torch.tensor(image).permute(2, 0, 1)\n\n    # Use this if processing grayscale\n    image = torch.tensor(image).unsqueeze(0)\n\n    if self.transform:\n        image = self.transform(image)\n\n    return image, label","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load dataset\ndataset = ASLDataset(root_dir=dataset_path, transform=None)\n\n# 80/20 train-test split\ntrain_size = int(0.8 * len(dataset))\ntest_size = len(dataset) - train_size\ntrain_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n\n# Create DataLoaders\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)\n\n# Show image\ndef imshow(img):\n    img = img / 2 + 0.5     # Unnormalize\n    npimg = img.numpy()\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n    plt.show()\n\n# Get a batch to display for testing\ndataiter = iter(train_loader)\nimages, labels = next(dataiter)\n\n# Show images\nimshow(torchvision.utils.make_grid(images))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_classes = 29   # A - Z, space, nothing, del\n\nclass SimpleCNN(nn.Module):\n    def __init__(self, num_classes):\n\n        super(SimpleCNN, self).__init__()\n\n        # Don't forget to change the number of input channels if using color images\n        self.conv1 = nn.Conv2d(1, 16, 3, padding=1)        # Convolutional layer (in_channels, out_channels, kernel_size, padding)\n        self.pool = nn.MaxPool2d(2, 2)                     # Pooling/downsampling layer\n        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)       # Second convolutional layer\n        self.fc1 = nn.Linear(32 * 16 * 16, 128)            # Fully-connected layer\n        self.fc2 = nn.Linear(128, num_classes)       \n\n    def forward(self, x):\n      \n        x = self.pool(torch.relu(self.conv1(x)))   \n        x = self.pool(torch.relu(self.conv2(x)))  \n        x = x.view(x.shape[0], -1)    # \n        x = torch.relu(self.fc1(x))    \n        x = self.fc2(x)      \n        return x\n\nmodel = SimpleCNN(num_classes=num_classes)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Basic training loop\nnum_epochs = 5\nfor epoch in range(num_epochs):\n\n  for i, (images, labels) in enumerate(train_loader):\n\n    #print(f\"Processing batch {i}/{len(train_loader)}\")\n\n    optimizer.zero_grad()\n    outputs = model(images)\n    loss = criterion(outputs, labels)\n    loss.backward()\n    optimizer.step()\n    \n  print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Single example prediction\nwith torch.no_grad():\n    \n    sample_image, true_label = dataset[200]    # Get sample image and its true label\n    sample_image = sample_image.unsqueeze(0)   # Add batch dimension\n    \n    # Predict label\n    prediction = model(sample_image)\n    predicted_label = torch.argmax(prediction).item()\n\n# Get the true label from the dataset's index_to_label mapping\ntrue_label_str = dataset.index_to_label[true_label]\n\n# Print the results\nprint(f\"True character: {true_label_str}\")\nprint(f\"Predicted character: {dataset.index_to_label[predicted_label]}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Set model to evaluation mode\nmodel.eval()\n\nall_true_labels = []\nall_predicted_labels = []\n\ntotal_loss = 0.0\ncorrect_preds = 0\ntotal_samples = 0\n\n# Disable gradients\nwith torch.no_grad():\n    \n    for images, labels in test_loader:\n        \n        # Forward pass\n        outputs = model(images)\n        \n        # Compute loss\n        loss = criterion(outputs, labels)\n        total_loss += loss.item()  # Accumulate the loss\n        \n        # Get predictions\n        _, predicted = torch.max(outputs, 1)\n        \n        # Store true and predicted labels\n        all_true_labels.extend(labels.cpu().numpy())  # move to CPU and convert to numpy\n        all_predicted_labels.extend(predicted.cpu().numpy())\n        \n        # Track num correct predictions\n        correct_preds += (predicted == labels).sum().item()\n        total_samples += labels.size(0)\n\n# Compute avg loss and accuracy\navg_loss = total_loss / len(test_loader)\naccuracy = 100 * correct_preds / total_samples\n\nprint(f\"Test Loss: {avg_loss:.4f}\")\nprint(f\"Test Accuracy: {accuracy:.2f}%\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Generate and plot confusion matrix\ncm = confusion_matrix(all_true_labels, all_predicted_labels)\n\nplt.figure(figsize=(10, 8))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=dataset.index_to_label.values(), yticklabels=dataset.index_to_label.values())\nplt.title('Confusion Matrix')\nplt.xlabel('Predicted Label')\nplt.ylabel('True Label')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}